
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Residual Policy Learning | residual-policy-learning</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Residual Policy Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Residual policy learning" />
<meta property="og:description" content="Residual policy learning" />
<link rel="canonical" href="https://k-r-allen.github.io/residual-policy-learning/" />
<meta property="og:url" content="https://k-r-allen.github.io/residual-policy-learning/" />
<meta property="og:site_name" content="residual-policy-learning" />
<script type="application/ld+json">
{"headline":"Residual Policy Learning","@type":"WebSite","url":"https://k-r-allen.github.io/residual-policy-learning/","name":"residual-policy-learning","description":"Residual policy learning","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="/residual-policy-learning/assets/css/style.css?v=b323e4921cd489a27b7a43526adab58a3c1c780e">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">residual-policy-learning</h1>
      <h2 class="project-tagline">Residual policy learning</h2>
      
    </section>

    <section class="main-content">
      <h1 id="residual-policy-learning">Residual Policy Learning</h1>
<h2 id="tom-silver-kelsey-allen-josh-tenenbaum-leslie-kaelbling">Tom Silver*, Kelsey Allen*, Josh Tenenbaum, Leslie Kaelbling</h2>

<p>We present Residual Policy Learning (RPL): a simple method for improving nondifferentiable policies using model-free deep reinforcement learning. RPL thrives in complex robotic manipulation tasks where good but imperfect controllers are available. In these tasks, reinforcement learning from scratch remains data-inefficient or intractable, but learning a <strong>residual</strong> on top of the initial controller can yield substantial improvement. We study RPL in five challenging MuJoCo tasks involving partial observability, sensor noise, model misspecification, and controller miscalibration. By combining learning with control algorithms, RPL can perform long-horizon, sparse-reward tasks for which reinforcement learning alone fails. Moreover, we find that RPL consistently and substantially improves on the initial controllers. We argue that RPL is a promising approach for combining the complementary strengths of deep reinforcement learning and robotic control, pushing the boundaries of what either can achieve independently</p>

<h2 id="link-to-the-arxiv">Link to the arXiv</h2>
<p><a href="http://arxiv.org/abs/1812.06298">http://arxiv.org/abs/1812.06298</a></p>

        <h2 id="video">Residual Policy Learning Video</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/MPqb2I-NrJ0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/k-r-allen/residual-policy-learning">residual-policy-learning</a> is maintained by <a href="https://github.com/k-r-allen">k-r-allen</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
